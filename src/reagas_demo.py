import os
from datasets import Dataset
from ragas import evaluate

# è®¾ç½®é˜¿é‡Œäº‘APIå¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
os.environ["DASHSCOPE_API_KEY"] = ""  # æ›¿æ¢ä¸ºä½ çš„å¯†é’¥

# ä¸»è¦è¯„ä¼°æŒ‡æ ‡åˆ†ç±»
from ragas.metrics import (
    # æ£€ç´¢è´¨é‡æŒ‡æ ‡
    context_precision,  # ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
    context_recall,  # ä¸Šä¸‹æ–‡å¬å›ç‡
    # ç”Ÿæˆè´¨é‡æŒ‡æ ‡
    faithfulness,  # ç­”æ¡ˆå¿ å®åº¦
    answer_relevancy,  # ç­”æ¡ˆç›¸å…³æ€§
    answer_correctness,  # ç­”æ¡ˆæ­£ç¡®æ€§
    answer_similarity  # ç­”æ¡ˆç›¸ä¼¼åº¦

)

def explain_metrics():
    """è§£é‡Šæ¯ä¸ªæŒ‡æ ‡çš„å«ä¹‰"""
    metrics_explanation = {
        # user_inputï¼šç”¨æˆ·çš„é—®é¢˜
        # retrieved_contextsï¼šæ£€ç´¢å‡ºæ¥çš„ä¸Šä¸‹æ–‡ç‰‡æ®µï¼ˆä¸€ä¸ªåˆ—è¡¨ï¼‰
        # referenceï¼šå‚è€ƒç­”æ¡ˆï¼ˆground truthï¼‰
        'context_precision': 'æ£€ç´¢çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç²¾ç¡®ç›¸å…³ï¼ˆé¿å…æ— å…³ä¿¡æ¯ï¼‰', # æ˜¯å¦ç›¸å…³ã€æ’åºä½ç½®ï¼Œ kä½ç½®å‰ç›¸å…³æ–‡æ¡£ä¹¦/kä½ç½®å‰æ–‡æ¡£æ€»æ•° ï¼Œæ‰€æœ‰çš„æ–‡æ¡£å–å¹³å‡å€¼
        # Context Precision = âˆ‘ (Precision@k Ã— rel_k) / Total relevant documents

        'context_recall': 'æ˜¯å¦æ£€ç´¢åˆ°äº†æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼ˆé¿å…ä¿¡æ¯é—æ¼ï¼‰', # ç­”æ¡ˆæ˜¯å¦éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡æ‰¾åˆ°æ”¯æ’‘
        'faithfulness': 'ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å¿ å®äºæä¾›çš„ä¸Šä¸‹æ–‡ï¼ˆé¿å…ç¼–é€ ï¼‰',
        'answer_relevancy': 'ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†é—®é¢˜', # ç­”æ¡ˆç›¸å…³æ€§
        'answer_correctness': 'ç­”æ¡ˆä¸çœŸå®ç­”æ¡ˆçš„åŒ¹é…ç¨‹åº¦', # ç­”æ¡ˆçœŸå®æ€§
        'answer_similarity': 'ç­”æ¡ˆä¸çœŸå®ç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦', # ç­”æ¡ˆç›¸ä¼¼åº¦
        'harmfulness': 'ç­”æ¡ˆæ˜¯å¦åŒ…å«æœ‰å®³å†…å®¹',
        'ragas_score': 'ç»¼åˆè¯„åˆ†ï¼ˆå¤šä¸ªæŒ‡æ ‡çš„åŠ æƒå¹³å‡ï¼‰'
    }

    print("ğŸ¯ Ragasè¯„ä¼°æŒ‡æ ‡è¯´æ˜:")
    for metric, desc in metrics_explanation.items():
        print(f"ğŸ“ {metric}: {desc}")


explain_metrics()
def simple_ragas_evaluation(test_data, sample_size=5):
    """
    æœ€ç®€å•çš„RAGè¯„ä¼°MVP
    """
    try:
        # 1. è¯»å–æ•°æ®
        print("ğŸ“Š è¯»å–æ•°æ®...")
       # df = pd.read_csv(csv_file_path)

        # 2. å‡†å¤‡Ragasæ•°æ®é›†
        print("ğŸ”„ å‡†å¤‡è¯„ä¼°æ•°æ®é›†...")

        # âœ… æ„å»º HuggingFace Dataset
        dataset = Dataset.from_dict(test_data)
        print(dataset)

        # 3. å¯¼å…¥æ¨¡å‹ï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨é˜¿é‡Œäº‘ï¼‰
        from langchain_community.llms import Tongyi
        from langchain_community.embeddings import DashScopeEmbeddings

        llm = Tongyi(model="qwen-turbo")
        embeddings = DashScopeEmbeddings(model="text-embedding-v3")
        # å­˜å‚¨æ¯æ¡é—®é¢˜ç»“æœ
        all_results = []

        # 4. æ‰§è¡Œè¯„ä¼°
        print("ğŸš€ å¼€å§‹è¯„ä¼°...")
        for idx, sample in enumerate(dataset):
            contexts_combined = " ".join(sample['contexts'])

            single_dataset = Dataset.from_dict({
                'question': [sample['question']],
                'contexts': [sample['contexts']],
                'answer': [sample['answer']],
                'ground_truth': [sample['ground_truth']]
            })

            result = evaluate(
                single_dataset,
                metrics=[context_precision, context_recall, faithfulness, answer_relevancy,
                         answer_correctness, answer_similarity],
                embeddings=embeddings,
                llm=llm
            )

            # answer_relevancy,  # ç­”æ¡ˆç›¸å…³æ€§
            # answer_correctness,  # ç­”æ¡ˆæ­£ç¡®æ€§
            # answer_similarity  # ç­”æ¡ˆç›¸ä¼¼åº¦
            row = result.to_pandas().iloc[0].to_dict()
            row['question'] = sample['question']
            all_results.append(row)

        # 5. è¾“å‡ºç»“æœ
        print("\nâœ… è¯„ä¼°å®Œæˆï¼")
        print("=" * 50)
        print("è¯„ä¼°ç»“æœ:")
        print(result)

        # ä¿å­˜ç»“æœ
        result_df = result.to_pandas()
        result_df.to_csv("mvp_evaluation_results.csv", index=False)
        print("\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: mvp_evaluation_results.csv")

        return result

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return None


if __name__ == "__main__":
    # ä½¿ç”¨æ–¹æ³•
    # å®Œæ•´æµ‹è¯•æ•°æ®é›†ï¼Œ10æ¡ç‹¬ç«‹é—®é¢˜ï¼Œæ¯æ¡ä¸Šä¸‹æ–‡åŒ…å«å¤šä¸ªæ–‡æœ¬å—
    # æ¯ä¸ªkeyéƒ½å¯ä»¥æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œä½†æ˜¯å¤šä¸ªé—®é¢˜å¿…é¡»é¡ºåºä¸€è‡´
    #ä¾‹å­ï¼š
    # data_samples = {
    #     'question': ['ç¬¬ä¸€å±Šè¶…çº§ç¢—æ˜¯ä»€ä¹ˆæ—¶å€™ä¸¾è¡Œçš„ï¼Ÿ', 'è°èµ¢å¾—äº†æœ€å¤šçš„è¶…çº§ç¢—å† å†›ï¼Ÿ'],
    #     'answer': ['ç¬¬ä¸€å±Šè¶…çº§ç¢—äº1967å¹´1æœˆ15æ—¥ä¸¾è¡Œ', 'èµ¢å¾—æœ€å¤šè¶…çº§ç¢—å† å†›çš„æ˜¯æ–°è‹±æ ¼å…°çˆ±å›½è€…é˜Ÿ'],
    #     'contexts': [['ç¬¬ä¸€å±Š AFL-NFL ä¸–ç•Œå† å†›èµ›æ˜¯ä¸€åœºç¾å¼æ©„æ¦„çƒæ¯”èµ›ï¼Œäº1967å¹´1æœˆ15æ—¥åœ¨æ´›æ‰çŸ¶çºªå¿µä½“è‚²é¦†ä¸¾è¡Œ'],
    #                  ['ç»¿æ¹¾åŒ…è£…å·¥é˜Ÿ...ä½äºå¨æ–¯åº·æ˜Ÿå·ç»¿æ¹¾å¸‚ã€‚', 'åŒ…è£…å·¥é˜Ÿå‚åŠ ...å…¨å›½æ©„æ¦„çƒè”åˆä¼šæ¯”èµ›']],
    #     'ground_truth': ['ç¬¬ä¸€å±Šè¶…çº§ç¢—äº1967å¹´1æœˆ15æ—¥ä¸¾è¡Œ', 'æ–°è‹±æ ¼å…°çˆ±å›½è€…é˜Ÿèµ¢å¾—äº†åˆ›çºªå½•çš„å…­æ¬¡è¶…çº§ç¢—å† å†›']
    # }

    #  contextsä¸Šä¸‹æ–‡å¿…é¡»ç»„æˆä¸€ä¸ªå­—ç¬¦ä¸²æ–‡æœ¬å—ï¼Œä¸èƒ½ä¼ ä¸€ä¸ªæ•°ç»„è¿›æ¥
    test_data = {
        'question': ['æœºå™¨å­¦ä¹ çš„ä¸»è¦åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ'],
        'contexts': [[
            "æœºå™¨å­¦ä¹ é€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹åœ¨é‡‘èé£æ§åº”ç”¨é¢†åŸŸä¸­ç”¨äºé£é™©é¢„æµ‹ã€‚"
            "æœºå™¨å­¦ä¹ åœ¨åŒ»ç–—è¯Šæ–­åº”ç”¨é¢†åŸŸä¸­å¯ä»¥å¸®åŠ©è¯†åˆ«ç–¾ç—…æ¨¡å¼ã€‚"
            "æœºå™¨å­¦ä¹ æ¨èç³»ç»Ÿé€šè¿‡ç”¨æˆ·è¡Œä¸ºæ•°æ®è¿›è¡Œä¸ªæ€§åŒ–æ¨èã€‚"
            "æœºå™¨å­¦ä¹ å›¾åƒè¯†åˆ«æ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦åº”ç”¨ä¹‹ä¸€ã€‚"
        ]],
        'answer': ['æœºå™¨å­¦ä¹ åº”ç”¨äºé‡‘èé£æ§ã€åŒ»ç–—è¯Šæ–­ã€æ¨èç³»ç»Ÿå’Œå›¾åƒè¯†åˆ«ã€‚'],
        'ground_truth': ['æœºå™¨å­¦ä¹ é€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹å¹¶åº”ç”¨äºé‡‘èé£æ§ã€åŒ»ç–—ã€æ¨èç³»ç»Ÿå’Œå›¾åƒè¯†åˆ«ç­‰é¢†åŸŸã€‚']
    }
    simple_ragas_evaluation(test_data, sample_size=3)  # å…ˆç”¨3æ¡æµ‹è¯•
